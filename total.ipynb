{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dad06c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'images'\n",
    "#folder_name = '/content/drive/MyDrive/Medical image Reporting/pickle_files'\n",
    "tr_file_name = 'train.pkl'\n",
    "te_file_name = 'test.pkl'\n",
    "chexnet_weights = 'brucechou1983_CheXNet_Keras_0.3.0_weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "482083c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import joblib\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D, Input, Embedding, LSTM,Dot,Reshape,Concatenate,BatchNormalization, GlobalMaxPooling2D, Dropout, Add, MaxPooling2D, GRU, AveragePooling2D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9a002f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chexnet weights ; https://drive.google.com/file/d/19BllaOvs2x5PLV_vlWMy4i8LapLb2j6b/view\n",
    "def create_chexnet(chexnet_weights = chexnet_weights,input_size=(224,224)):\n",
    "    \"\"\"\n",
    "    chexnet_weights: weights value in .h5 format of chexnet\n",
    "    creates a chexnet model with preloaded weights present in chexnet_weights file\n",
    "    \"\"\"\n",
    "    model = tf.keras.applications.DenseNet121(include_top=False,input_shape = input_size+(3,)) #importing densenet the last layer will be a relu activation layer\n",
    "\n",
    "  #we need to load the weights so setting the architecture of the model as same as the one of the chexnet\n",
    "    x = model.output #output from chexnet\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(14, activation=\"sigmoid\", name=\"chexnet_output\")(x) #here activation is sigmoid as seen in research paper\n",
    "\n",
    "    chexnet = tf.keras.Model(inputs = model.input,outputs = x)\n",
    "    chexnet.load_weights(chexnet_weights)\n",
    "    chexnet = tf.keras.Model(inputs = model.input,outputs = chexnet.layers[-3].output)  #we will be taking the 3rd last layer (here it is layer before global avgpooling)\n",
    "    #since we are using attention here\n",
    "    return chexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c8eebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,name = \"image_encoder_block\"):\n",
    "        super().__init__()\n",
    "        self.chexnet = create_chexnet(input_size = (224,224))\n",
    "        self.chexnet.trainable = False\n",
    "        self.avgpool = AveragePooling2D()\n",
    "        # for i in range(10): #the last 10 layers of chexnet will be trained\n",
    "        #   self.chexnet.layers[-i].trainable = True\n",
    "    \n",
    "    def call(self,data):\n",
    "        op = self.chexnet(data) #op shape: (None,7,7,1024)\n",
    "        op = self.avgpool(op) #op shape (None,3,3,1024)\n",
    "        op = tf.reshape(op,shape = (-1,op.shape[1]*op.shape[2],op.shape[3])) #op shape: (None,9,1024)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a46c792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(image1,image2,dense_dim,dropout_rate):\n",
    "\n",
    "    im_encoder = Image_encoder()\n",
    "    bkfeat1 = im_encoder(image1) #shape: (None,9,1024)\n",
    "    bk_dense = Dense(dense_dim,name = 'bkdense',activation = 'relu') #shape: (None,9,512)\n",
    "    bkfeat1 = bk_dense(bkfeat1)\n",
    "\n",
    "  #image2\n",
    "    bkfeat2 = im_encoder(image2) #shape: (None,9,1024)\n",
    "    bkfeat2 = bk_dense(bkfeat2) #shape: (None,9,512)\n",
    "\n",
    "\n",
    "  #combining image1 and image2\n",
    "    concat = Concatenate(axis=1)([bkfeat1,bkfeat2]) #concatenating through the second axis shape: (None,18,1024)\n",
    "    bn = BatchNormalization(name = \"encoder_batch_norm\")(concat) \n",
    "    dropout = Dropout(dropout_rate,name = \"encoder_dropout\")(bn)\n",
    "    return dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "68b9c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class global_attention(tf.keras.layers.Layer):\n",
    "    def __init__(self,dense_dim):\n",
    "        super().__init__()\n",
    "        # Intialize variables needed for Concat score function here\n",
    "        self.W1 = Dense(units = dense_dim) #weight matrix of shape enc_units*dense_dim\n",
    "        self.W2 = Dense(units = dense_dim) #weight matrix of shape dec_units*dense_dim\n",
    "        self.V = Dense(units = 1) #weight matrix of shape dense_dim*1 \n",
    "        #op (None,98,1)\n",
    "\n",
    "\n",
    "    def call(self,encoder_output,decoder_h): #here the encoded output will be the concatted image bk features shape: (None,98,dense_dim)\n",
    "        decoder_h = tf.expand_dims(decoder_h,axis=1) #shape: (None,1,dense_dim)\n",
    "        tanh_input = self.W1(encoder_output) + self.W2(decoder_h) #ouput_shape: batch_size*98*dense_dim\n",
    "        tanh_output =  tf.nn.tanh(tanh_input)\n",
    "        attention_weights = tf.nn.softmax(self.V(tanh_output),axis=1) #shape= batch_size*98*1 getting attention alphas\n",
    "        op = attention_weights*encoder_output#op_shape: batch_size*98*dense_dim  multiply all aplhas with corresponding context vector\n",
    "        context_vector = tf.reduce_sum(op,axis=1) #summing all context vector over the time period ie input length, output_shape: batch_size*dense_dim\n",
    "\n",
    "\n",
    "        return context_vector,attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "472f049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class One_Step_Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,vocab_size, embedding_dim, max_pad, dense_dim ,name = \"onestepdecoder\"):\n",
    "        # Initialize decoder embedding layer, LSTM and any other objects needed\n",
    "        super().__init__()\n",
    "        self.dense_dim = dense_dim\n",
    "        self.embedding = Embedding(input_dim = vocab_size+1,\n",
    "                                output_dim = embedding_dim,\n",
    "                                input_length=max_pad,\n",
    "                                mask_zero=True, \n",
    "                                name = 'onestepdecoder_embedding'\n",
    "                              )\n",
    "        self.LSTM = GRU(units=self.dense_dim,\n",
    "                    # return_sequences=True,\n",
    "                    return_state=True,\n",
    "                    name = 'onestepdecoder_LSTM'\n",
    "                    )\n",
    "        self.attention = global_attention(dense_dim = dense_dim)\n",
    "        self.concat = Concatenate(axis=-1)\n",
    "        self.dense = Dense(dense_dim,name = 'onestepdecoder_embedding_dense',activation = 'relu')\n",
    "        self.final = Dense(vocab_size+1,activation='softmax')\n",
    "        self.concat = Concatenate(axis=-1)\n",
    "        self.add =Add()\n",
    "        \n",
    "    def call(self,input_to_decoder, encoder_output, decoder_h):#,decoder_c):\n",
    "        '''\n",
    "            One step decoder mechanisim step by step:\n",
    "            A. Pass the input_to_decoder to the embedding layer and then get the output(batch_size,1,embedding_dim)\n",
    "            B. Using the encoder_output and decoder hidden state, compute the context vector.\n",
    "            C. Concat the context vector with the step A output\n",
    "            D. Pass the Step-C output to LSTM/GRU and get the decoder output and states(hidden and cell state)\n",
    "            E. Pass the decoder output to dense layer(vocab size) and store the result into output.\n",
    "            F. Return the states from step D, output from Step E, attention weights from Step -B\n",
    "\n",
    "           here state_h,state_c are decoder states\n",
    "        '''\n",
    "        embedding_op = self.embedding(input_to_decoder) #output shape = batch_size*1*embedding_shape (only 1 token)\n",
    "    \n",
    "\n",
    "        context_vector,attention_weights = self.attention(encoder_output,decoder_h) #passing hidden state h of decoder and encoder output\n",
    "        #context_vector shape: batch_size*dense_dim we need to add time dimension\n",
    "        context_vector_time_axis = tf.expand_dims(context_vector,axis=1)\n",
    "        #now we will combine attention output context vector with next word input to the lstm here we will be teacher forcing\n",
    "        concat_input = self.concat([context_vector_time_axis,embedding_op])#output dimension = batch_size*input_length(here it is 1)*(dense_dim+embedding_dim)\n",
    "    \n",
    "        output,decoder_h = self.LSTM(concat_input,initial_state = decoder_h)\n",
    "        #output shape = batch*1*dense_dim and decoder_h,decoder_c has shape = batch*dense_dim\n",
    "        #we need to remove the time axis from this decoder_output\n",
    "    \n",
    "\n",
    "        output = self.final(output)#shape = batch_size*decoder vocab size\n",
    "        return output,decoder_h,attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c5c713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(tf.keras.Model):\n",
    "  \"\"\"\n",
    "  Decodes the encoder output and caption\n",
    "  \"\"\"\n",
    "  def __init__(self,max_pad, embedding_dim,dense_dim,batch_size ,vocab_size):\n",
    "    super().__init__()\n",
    "    self.onestepdecoder = One_Step_Decoder(vocab_size = vocab_size, embedding_dim = embedding_dim, max_pad = max_pad, dense_dim = dense_dim)\n",
    "    self.output_array = tf.TensorArray(tf.float32,size=max_pad)\n",
    "    self.max_pad = max_pad\n",
    "    self.batch_size = batch_size\n",
    "    self.dense_dim =dense_dim\n",
    "    \n",
    "  @tf.function\n",
    "  def call(self,encoder_output,caption):#,decoder_h,decoder_c): #caption : (None,max_pad), encoder_output: (None,dense_dim)\n",
    "    decoder_h, decoder_c = tf.zeros_like(encoder_output[:,0]), tf.zeros_like(encoder_output[:,0]) #decoder_h, decoder_c\n",
    "    output_array = tf.TensorArray(tf.float32,size=self.max_pad)\n",
    "    for timestep in range(self.max_pad): #iterating through all timesteps ie through max_pad\n",
    "        output,decoder_h,attention_weights = self.onestepdecoder(caption[:,timestep:timestep+1], encoder_output, decoder_h)\n",
    "        output_array = output_array.write(timestep,output) #timestep*batch_size*vocab_size\n",
    "\n",
    "    self.output_array = tf.transpose(output_array.stack(),[1,0,2]) #.stack :Return the values in the TensorArray as a stacked Tensor.)\n",
    "        #shape output_array: (batch_size,max_pad,vocab_size)\n",
    "    return self.output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e80e9891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "    creates the best model ie the attention model\n",
    "    and returns the model after loading the weights\n",
    "    and also the tokenizer\n",
    "    \"\"\"\n",
    "  #hyperparameters\n",
    "    input_size = (224,224)\n",
    "    tokenizer = joblib.load('tokenizer.pkl')\n",
    "    max_pad = 29\n",
    "    batch_size = 100\n",
    "    vocab_size = len(tokenizer.word_index)\n",
    "    embedding_dim = 100\n",
    "    dense_dim = 512\n",
    "    lstm_units = dense_dim\n",
    "    dropout_rate = 0.2\n",
    "\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    image1 = Input(shape = (input_size + (3,))) #shape = 224,224,3\n",
    "    image2 = Input(shape = (input_size + (3,))) #https://www.w3resource.com/python-exercises/tuple/python-tuple-exercise-5.php\n",
    "    caption = Input(shape = (max_pad,))\n",
    "\n",
    "    encoder_output = encoder(image1,image2,dense_dim,dropout_rate) #shape: (None,28,512)\n",
    "\n",
    "    output = decoder(max_pad, embedding_dim,dense_dim,batch_size ,vocab_size)(encoder_output,caption)\n",
    "    model = tf.keras.Model(inputs = [image1,image2,caption], outputs = output)\n",
    "    model_filename = 'attention.h5'\n",
    "    model_save = os.path.join(model_filename)\n",
    "    model.load_weights(model_save)\n",
    "\n",
    "    return model,tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "73936fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_search_predict(image1,image2,model,tokenizer,input_size = (224,224)):\n",
    "    \"\"\"\n",
    "    Given paths to two x-ray images predicts the impression part of the x-ray in a greedy search algorithm\n",
    "    \"\"\"\n",
    "    image1 = tf.expand_dims(cv2.resize(image1,input_size,interpolation = cv2.INTER_NEAREST),axis=0) #introduce batch and resize\n",
    "    image2 = tf.expand_dims(cv2.resize(image2,input_size,interpolation = cv2.INTER_NEAREST),axis=0)\n",
    "    image1 = model.get_layer('image_encoder')(image1)\n",
    "    image2 = model.get_layer('image_encoder')(image2)\n",
    "    image1 = model.get_layer('bkdense')(image1)\n",
    "    image2 = model.get_layer('bkdense')(image2)\n",
    "\n",
    "    concat = model.get_layer('concatenate')([image1,image2])\n",
    "    enc_op = model.get_layer('encoder_batch_norm')(concat)  \n",
    "    enc_op = model.get_layer('encoder_dropout')(enc_op) #this is the output from encoder\n",
    "\n",
    "\n",
    "    decoder_h,decoder_c = tf.zeros_like(enc_op[:,0]),tf.zeros_like(enc_op[:,0])\n",
    "    a = []\n",
    "    pred = []\n",
    "    max_pad = 29\n",
    "    for i in range(max_pad):\n",
    "        if i==0: #if first word\n",
    "            caption = np.array(tokenizer.texts_to_sequences(['<cls>'])) #shape: (1,1)\n",
    "        output,decoder_h,attention_weights = model.get_layer('decoder').onestepdecoder(caption,enc_op,decoder_h)#,decoder_c) decoder_c,\n",
    "\n",
    "    #prediction\n",
    "        max_prob = tf.argmax(output,axis=-1)  #tf.Tensor of shape = (1,1)\n",
    "        caption = np.array([max_prob]) #will be sent to onstepdecoder for next iteration\n",
    "        if max_prob==np.squeeze(tokenizer.texts_to_sequences(['<end>'])): \n",
    "            break;\n",
    "        else:\n",
    "            a.append(tf.squeeze(max_prob).numpy())\n",
    "    return tokenizer.sequences_to_texts([a])[0] #here output would be 1,1 so subscripting to open the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a56006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict1(image1,image2=None,model_tokenizer = None):\n",
    "    \"\"\"given image1 and image 2 filepaths returns the predicted caption,\n",
    "    the model_tokenizer will contain stored model_weights and tokenizer \n",
    "    \"\"\"\n",
    "    if image2 == None: #if only 1 image file is given\n",
    "        image2 = image1\n",
    "\n",
    "    try:\n",
    "        image1 = cv2.imread(image1,cv2.IMREAD_UNCHANGED)/255 \n",
    "        image2 = cv2.imread(image2,cv2.IMREAD_UNCHANGED)/255\n",
    "    except:\n",
    "        return print(\"Must be an image\")\n",
    "\n",
    "    if model_tokenizer == None:\n",
    "        model,tokenizer = create_model()\n",
    "    else:\n",
    "        model,tokenizer = model_tokenizer[0],model_tokenizer[1]\n",
    "    predicted_caption = greedy_search_predict(image1,image2,model,tokenizer)\n",
    "\n",
    "    return predicted_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9cf847c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function1(image1,image2):\n",
    "    \"\"\"\n",
    "    here image1 and image2 will be a list of image\n",
    "    filepaths and outputs the resulting captions as a list\n",
    "    \"\"\"\n",
    "    model_tokenizer = list(create_model())\n",
    "    predicted_caption = []\n",
    "    for i1,i2 in zip(image1,image2):\n",
    "        caption = predict1(i1,i2,model_tokenizer)\n",
    "        predicted_caption.append(caption)\n",
    "\n",
    "    return predicted_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81972556",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(os.path.join(tr_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b3156042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['heart size normal . lungs are clear .', 'stable mild cardiomegaly and mild bilateral interstitial opacities which represent mild pulmonary edema .']\n",
      "len 2\n",
      "Wall time: 2min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "k = [7,300]\n",
    "image1,image2 = df.iloc[k]['image_1'].values,df.iloc[k]['image_2'].values\n",
    "result = function1(image1,image2)\n",
    "print(result)\n",
    "print(\"len\",len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "278ac67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle(os.path.join(te_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "166a2742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_caption:\n",
      " left picc in the mid svc . negative for pneumothorax . unchanged bibasilar airspace opacities compatible with pulmonary effusions and atelectasis . increased interstitial prominence may reflect underlying pulmonary edema possibly secondary to infectious etiology . stable postsurgical changes of the distal right clavicle .\n",
      "predicted_caption:\n",
      " bilateral pleural effusions right larger than left . early interstitial show pulmonary edema .\n",
      "Wall time: 3min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "k = 562\n",
    "caption,image1,image2 = [test['impression'][k]],[test['image_1'][k]],[test['image_2'][k]]\n",
    "predicted_caption = function1(image1,image2)\n",
    "print(\"true_caption:\\n\",caption[0])\n",
    "print(\"predicted_caption:\\n\",predicted_caption[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "945b08f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_caption:\n",
      " stable chest without acute cardiopulmonary abnormality .\n",
      "predicted_caption:\n",
      " heart size is normal . lungs are clear .\n",
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "k = 500\n",
    "caption,image1,image2 = [test['impression'][k]],[test['image_1'][k]],[test['image_2'][k]]\n",
    "predicted_caption = function1(image1,image2)\n",
    "print(\"true_caption:\\n\",caption[0])\n",
    "print(\"predicted_caption:\\n\",predicted_caption[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9175e39d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
